{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/cassava-leaf-disease-classification/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-41034dcf9f5a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_seed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mSEED\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'../input/cassava-leaf-disease-classification/train.csv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[0;32m    684\u001B[0m     )\n\u001B[0;32m    685\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 686\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    687\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    688\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    451\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 452\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfp_or_buf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    453\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    454\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    934\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    935\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 936\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    937\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    938\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1166\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"c\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1167\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"c\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1168\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCParserWrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1169\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1170\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"python\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m   1996\u001B[0m         \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"usecols\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0musecols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1997\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1998\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparsers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTextReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1999\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munnamed_cols\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munnamed_cols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2000\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../input/cassava-leaf-disease-classification/train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten,GlobalAveragePooling2D,BatchNormalization, Activation, GlobalMaxPool2D\n",
    "import glob\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "SEED = 42\n",
    "DEBUG = False\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "df = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n",
    "df.head()\n",
    "\n",
    "df['path'] = '../input/cassava-leaf-disease-classification/train_images/' + df['image_id']\n",
    "df.label.value_counts(normalize=True) * 100\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    _, df = train_test_split(df, test_size = 0.1, random_state=SEED, shuffle=True, stratify=df['label'])\n",
    "\n",
    "\n",
    "X_train, X_valid = train_test_split(df, test_size = 0.1, random_state=SEED, shuffle=True)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train.path.values, X_train.label.values))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((X_valid.path.values, X_valid.label.values))\n",
    "\n",
    "for path, label in train_ds.take(5):\n",
    "    print ('Path: {}, Label: {}'.format(path, label))\n",
    "\n",
    "for path, label in valid_ds.take(5):\n",
    "    print ('Path: {}, Label: {}'.format(path, label))\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "target_size_dim = 512\n",
    "\n",
    "def process_data_train(image_path, label):\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.random_brightness(img, 0.3)\n",
    "    img = tf.image.random_flip_left_right(img, seed=None)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    img = tf.image.random_crop(img, size=[target_size_dim, target_size_dim, 3])\n",
    "    return img, label\n",
    "\n",
    "def process_data_valid(image_path, label):\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [target_size_dim,target_size_dim])\n",
    "    return img, label\n",
    "\n",
    "\n",
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "train_ds = train_ds.map(process_data_train, num_parallel_calls=AUTOTUNE)\n",
    "valid_ds = valid_ds.map(process_data_valid, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "for image, label in train_ds.take(1):\n",
    "    plt.imshow(image.numpy().astype('uint8'))\n",
    "    plt.show()\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())\n",
    "\n",
    "def configure_for_performance(ds, batch_size = 32):\n",
    "    ds = ds.cache('/kaggle/dump.tfcache')\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=1024)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_ds_batch = configure_for_performance(train_ds, batch_size)\n",
    "valid_ds_batch = valid_ds.batch(batch_size)\n",
    "\n",
    "image_batch, label_batch = next(iter(train_ds_batch))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(8):\n",
    "    ax = plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "    label = label_batch[i].numpy()\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2, interpolation='nearest'),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomContrast((0.2 ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(8):\n",
    "    augmented_images = data_augmentation(image_batch)\n",
    "    ax = plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(augmented_images[i].numpy().astype(\"uint8\"))\n",
    "    label = label_batch[i].numpy()\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "## Only available in tf2.3+\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "\n",
    "\n",
    "def load_pretrained_model(weights_path, drop_connect, target_size_dim, layers_to_unfreeze=5):\n",
    "    model = EfficientNetB4(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            drop_connect_rate=0.4\n",
    "        )\n",
    "\n",
    "\n",
    "    model.trainable = True\n",
    "\n",
    "    # for layer in model.layers[-layers_to_unfreeze:]:\n",
    "    #     if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "    #         layer.trainable = True\n",
    "\n",
    "    if DEBUG:\n",
    "        for layer in model.layers:\n",
    "            #print(layer.name, layer.trainable)\n",
    "            pass\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_my_model(base_model, optimizer, loss='sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy']):\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(target_size_dim, target_size_dim, 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    outputs_eff = base_model(x)\n",
    "    global_avg_pooling = GlobalMaxPool2D()(outputs_eff)\n",
    "    dense_1= Dense(256)(global_avg_pooling)\n",
    "    activation = Activation('relu')(dense_1)\n",
    "    bn_1 = BatchNormalization()(activation)\n",
    "    dp_1 = Dropout(0.3)(bn_1)\n",
    "    dense_2= Dense(128)(dp_1)\n",
    "    activation1 = Activation('relu')(dense_2)\n",
    "    bn_2 = BatchNormalization()(activation1)\n",
    "    dropout = Dropout(0.2)(bn_2)\n",
    "    dense_2 = Dense(5)(dropout)\n",
    "    outputs = Activation('softmax', dtype='float32', name='predictions')(dense_2)\n",
    "\n",
    "    my_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    my_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "\n",
    "    )\n",
    "    return my_model\n",
    "\n",
    "\n",
    "\n",
    "#!wget https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
    "## to get model weights\n",
    "\n",
    "model_weights_path = '../input/effnetb4-ns/effnetb4_ns.h5'\n",
    "model_weights_path\n",
    "\n",
    "drop_rate = 0.4 ## value of dropout to be used in loaded network\n",
    "base_model = load_pretrained_model( model_weights_path, drop_rate, target_size_dim )\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 1e-4)\n",
    "my_model = build_my_model(base_model, optimizer)\n",
    "my_model.summary()\n",
    "\n",
    "weight_path_save = 'best_model.hdf5'\n",
    "last_weight_path = 'last_model.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path_save,\n",
    "                             monitor= 'val_sparse_categorical_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode= 'min',\n",
    "                             save_weights_only = False)\n",
    "checkpoint_last = ModelCheckpoint(last_weight_path,\n",
    "                             monitor= 'val_sparse_categorical_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=False,\n",
    "                             mode= 'min',\n",
    "                             save_weights_only = False)\n",
    "\n",
    "\n",
    "early = EarlyStopping(monitor= 'val_sparse_categorical_accuracy',\n",
    "                      mode= 'min',\n",
    "                      patience=5)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_sparse_categorical_accuracy', factor=0.8, patience=2, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, checkpoint_last, early, reduceLROnPlat]\n",
    "\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)\n",
    "\n",
    "if DEBUG:\n",
    "    epochs = 3\n",
    "else:\n",
    "    epochs = 12\n",
    "\n",
    "print(f\"Model will train for {epochs} epochs\")\n",
    "\n",
    "if DEBUG:\n",
    "    history = my_model.fit(train_ds_batch,\n",
    "                              validation_data = valid_ds_batch,\n",
    "                              epochs = epochs,\n",
    "                              callbacks = callbacks_list,\n",
    "                               steps_per_epoch = 1,\n",
    "\n",
    "\n",
    "                              )\n",
    "else:\n",
    "    history = my_model.fit(train_ds_batch,\n",
    "                              validation_data = valid_ds_batch,\n",
    "                              epochs = epochs,\n",
    "                              callbacks = callbacks_list\n",
    "\n",
    "\n",
    "                              )\n",
    "\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    local_epochs = len(hist.history[\"sparse_categorical_accuracy\"])\n",
    "    plt.plot(np.arange(local_epochs, step=1), hist.history[\"sparse_categorical_accuracy\"], '-o', label='Train Accuracy',color='#ff7f0e')\n",
    "    plt.plot(np.arange(local_epochs, step=1), hist.history[\"val_sparse_categorical_accuracy\"], '-o',label='Val Accuracy',color='#1f77b4')\n",
    "    plt.xlabel('Epoch',size=14)\n",
    "    plt.ylabel('Accuracy',size=14)\n",
    "    plt.legend(loc=2)\n",
    "\n",
    "    plt2 = plt.gca().twinx()\n",
    "    plt2.plot(np.arange(local_epochs, step=1) ,history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
    "    plt2.plot(np.arange(local_epochs, step=1) ,history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n",
    "    plt.legend(loc=3)\n",
    "    plt.ylabel('Loss',size=14)\n",
    "    plt.title(\"Model Accuracy and loss\")\n",
    "\n",
    "    plt.savefig('loss.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(history)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "my_model.load_weights(weight_path_save) ## load the best model or all your metrics would be on the last run not on the best one\n",
    "\n",
    "pred_valid_y = my_model.predict(valid_ds_batch, workers=4, verbose = True)\n",
    "pred_valid_y_labels = np.argmax(pred_valid_y, axis=-1)\n",
    "\n",
    "valid_labels = np.concatenate([y.numpy() for x, y in valid_ds_batch], axis=0)\n",
    "\n",
    "\n",
    "print(classification_report(valid_labels, pred_valid_y_labels ))\n",
    "\n",
    "print(confusion_matrix(valid_labels, pred_valid_y_labels ))\n",
    "\n",
    "import glob\n",
    "\n",
    "test_images = glob.glob('../input/cassava-leaf-disease-classification/test_images/*.jpg')\n",
    "#test_images = test_images * 5\n",
    "print(test_images)\n",
    "\n",
    "df_test = pd.DataFrame(np.array(test_images), columns=['Path'])\n",
    "df_test.head()\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((df_test.Path.values))\n",
    "\n",
    "\n",
    "def process_test(image_path):\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.random_brightness(img, 0.3)\n",
    "    img = tf.image.random_flip_left_right(img, seed=None)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    img = tf.image.random_crop(img, size=[target_size_dim, target_size_dim, 3])\n",
    "    return img\n",
    "\n",
    "test_ds = test_ds.map(process_test, num_parallel_calls=AUTOTUNE).batch(batch_size*2)\n",
    "\n",
    "preds = []\n",
    "for i in range(5):\n",
    "\n",
    "    pred_test = my_model.predict(test_ds, workers=16, verbose=1)\n",
    "    preds.append(pred_test)\n",
    "\n",
    "\n",
    "pred_y = np.mean(preds, axis=0)\n",
    "\n",
    "#pred_y = my_model.predict(test_ds, workers=4)\n",
    "pred_y_argmax = np.argmax(pred_y, axis=-1)\n",
    "\n",
    "df_test['image_id'] = df_test.Path.str.split('/').str[-1]\n",
    "df_test['label'] = pred_y_argmax\n",
    "df_test= df_test[['image_id','label']]\n",
    "df_test.head()\n",
    "\n",
    "df_test.to_csv('submission.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}